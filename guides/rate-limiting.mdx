# Rate Limiting Guide

Understanding and handling rate limits effectively.

## Overview

Rate limits protect the API from overload and ensure fair resource allocation. All API endpoints have limits based on your organization tier.

### Rate Limit Factors

- **Requests per minute (RPM)** - How many requests you can make per minute
- **Tokens per day** - How many tokens you can process per day
- **Concurrent requests** - How many requests can run simultaneously

## Rate Limit Tiers

| Tier | RPM | Tokens/Day | Concurrent | Monthly Cost |
|------|-----|------------|------------|--------------|
| **Free** | 10 | 10,000 | 2 | $0 |
| **Standard** | 60 | 1,000,000 | 10 | $100 |
| **Premium** | 300 | 10,000,000 | 50 | $500 |
| **Enterprise** | Custom | Custom | Custom | Custom |

## Rate Limit Headers

Every API response includes rate limit information:

```http
HTTP/1.1 200 OK
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 45
X-RateLimit-Reset: 1730634120
```

| Header | Description |
|--------|-------------|
| `X-RateLimit-Limit` | Max requests per minute for your tier |
| `X-RateLimit-Remaining` | Requests remaining in current window |
| `X-RateLimit-Reset` | Unix timestamp when limit resets |

## Rate Limit Errors

### 429 Too Many Requests

When you exceed your rate limit:

```json
{
  "error": {
    "type": "rate_limit_exceeded",
    "message": "Rate limit exceeded. Retry after 30 seconds.",
    "code": "rate_limit_exceeded"
  }
}
```

Response headers:
```http
HTTP/1.1 429 Too Many Requests
Retry-After: 30
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1730634120
```

## Handling Rate Limits

### Basic Retry Logic

<CodeGroup>

```typescript TypeScript
async function makeRequestWithRetry(
  requestFn: () => Promise<any>,
  maxRetries = 3
): Promise<any> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await requestFn();
    } catch (error) {
      if (error.status === 429) {
        const retryAfter = parseInt(error.headers?.['retry-after'] || '30');
        console.log(`Rate limited. Retrying in ${retryAfter}s...`);
        await new Promise(resolve => setTimeout(resolve, retryAfter * 1000));
      } else {
        throw error;
      }
    }
  }
  throw new Error('Max retries exceeded');
}

// Usage
const completion = await makeRequestWithRetry(() =>
  client.chat.completions.create({
    model: 'grok-2',
    messages: [{ role: 'user', content: 'Hello' }]
  })
);
```

```python Python
import time

def make_request_with_retry(request_fn, max_retries=3):
    for attempt in range(max_retries):
        try:
            return request_fn()
        except Exception as error:
            if hasattr(error, 'status') and error.status == 429:
                retry_after = int(error.headers.get('retry-after', 30))
                print(f"Rate limited. Retrying in {retry_after}s...")
                time.sleep(retry_after)
            else:
                raise error
    raise Exception("Max retries exceeded")

# Usage
completion = make_request_with_retry(
    lambda: client.chat.completions.create(
        model="grok-2",
        messages=[{"role": "user", "content": "Hello"}]
    )
)
```

</CodeGroup>

### Exponential Backoff

Implement exponential backoff for more graceful retries:

<CodeGroup>

```typescript TypeScript
async function makeRequestWithBackoff(
  requestFn: () => Promise<any>,
  maxRetries = 5
): Promise<any> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await requestFn();
    } catch (error) {
      if (error.status === 429) {
        // Exponential backoff: 2^attempt * 1000ms
        const delay = Math.min(Math.pow(2, attempt) * 1000, 32000);
        console.log(`Rate limited. Retrying in ${delay / 1000}s...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      } else {
        throw error;
      }
    }
  }
  throw new Error('Max retries exceeded');
}

// Usage
const completion = await makeRequestWithBackoff(() =>
  client.chat.completions.create({
    model: 'grok-2',
    messages: [{ role: 'user', content: 'Hello' }]
  })
);
```

```python Python
import time

def make_request_with_backoff(request_fn, max_retries=5):
    for attempt in range(max_retries):
        try:
            return request_fn()
        except Exception as error:
            if hasattr(error, 'status') and error.status == 429:
                # Exponential backoff: 2^attempt * 1000ms
                delay = min(2 ** attempt * 1000, 32000) / 1000
                print(f"Rate limited. Retrying in {delay}s...")
                time.sleep(delay)
            else:
                raise error
    raise Exception("Max retries exceeded")

# Usage
completion = make_request_with_backoff(
    lambda: client.chat.completions.create(
        model="grok-2",
        messages=[{"role": "user", "content": "Hello"}]
    )
)
```

</CodeGroup>

### Check Rate Limit Before Request

Monitor rate limit headers to avoid hitting limits:

<CodeGroup>

```typescript TypeScript
class RateLimitedClient {
  private remaining: number = Infinity;
  private resetTime: number = 0;
  private client: OpenAI;

  constructor(apiKey: string) {
    this.client = new OpenAI({
      apiKey,
      baseURL: 'https://api.outcryai.com/v1'
    });
  }

  async makeRequest<T>(requestFn: () => Promise<T>): Promise<T> {
    // Wait if rate limit exhausted
    if (this.remaining <= 0 && Date.now() < this.resetTime) {
      const waitTime = this.resetTime - Date.now();
      console.log(`Rate limit exhausted. Waiting ${waitTime}ms...`);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }

    try {
      const response = await requestFn();

      // Update rate limit tracking
      // Note: This is simplified - in practice you'd parse headers from raw response
      return response;
    } catch (error) {
      if (error.status === 429) {
        const retryAfter = parseInt(error.headers?.['retry-after'] || '30');
        this.remaining = 0;
        this.resetTime = Date.now() + (retryAfter * 1000);
        throw error;
      }
      throw error;
    }
  }
}

// Usage
const client = new RateLimitedClient(process.env.OUTCRY_API_KEY);
const completion = await client.makeRequest(() =>
  client.chat.completions.create({
    model: 'grok-2',
    messages: [{ role: 'user', content: 'Hello' }]
  })
);
```

```python Python
import time
from datetime import datetime

class RateLimitedClient:
    def __init__(self, api_key):
        self.remaining = float('inf')
        self.reset_time = 0
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.outcryai.com/v1"
        )

    def make_request(self, request_fn):
        # Wait if rate limit exhausted
        if self.remaining <= 0 and time.time() < self.reset_time:
            wait_time = self.reset_time - time.time()
            print(f"Rate limit exhausted. Waiting {wait_time}s...")
            time.sleep(wait_time)

        try:
            response = request_fn()
            # Update rate limit tracking
            return response
        except Exception as error:
            if hasattr(error, 'status') and error.status == 429:
                retry_after = int(error.headers.get('retry-after', 30))
                self.remaining = 0
                self.reset_time = time.time() + retry_after
                raise error
            raise error

# Usage
client = RateLimitedClient(os.environ.get("OUTCRY_API_KEY"))
completion = client.make_request(
    lambda: client.client.chat.completions.create(
        model="grok-2",
        messages=[{"role": "user", "content": "Hello"}]
    )
)
```

</CodeGroup>

### Queue Requests

Implement a request queue to stay within limits:

<CodeGroup>

```typescript TypeScript
class RequestQueue {
  private queue: Array<() => Promise<any>> = [];
  private processing = false;
  private requestsPerMinute: number;
  private requestTimes: number[] = [];

  constructor(requestsPerMinute: number) {
    this.requestsPerMinute = requestsPerMinute;
  }

  async enqueue<T>(requestFn: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      this.queue.push(async () => {
        try {
          const result = await requestFn();
          resolve(result);
        } catch (error) {
          reject(error);
        }
      });

      if (!this.processing) {
        this.processQueue();
      }
    });
  }

  private async processQueue() {
    this.processing = true;

    while (this.queue.length > 0) {
      // Remove old request times (older than 1 minute)
      const oneMinuteAgo = Date.now() - 60000;
      this.requestTimes = this.requestTimes.filter(t => t > oneMinuteAgo);

      // Wait if we've hit the rate limit
      if (this.requestTimes.length >= this.requestsPerMinute) {
        const oldestRequest = this.requestTimes[0];
        const waitTime = 60000 - (Date.now() - oldestRequest);
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }

      // Process next request
      const requestFn = this.queue.shift();
      if (requestFn) {
        this.requestTimes.push(Date.now());
        await requestFn();
      }
    }

    this.processing = false;
  }
}

// Usage
const queue = new RequestQueue(60); // 60 RPM

for (let i = 0; i < 100; i++) {
  queue.enqueue(() =>
    client.chat.completions.create({
      model: 'grok-2',
      messages: [{ role: 'user', content: `Message ${i}` }]
    })
  ).then(completion => {
    console.log(`Completed request ${i}`);
  });
}
```

```python Python
import time
import asyncio
from collections import deque

class RequestQueue:
    def __init__(self, requests_per_minute):
        self.queue = deque()
        self.processing = False
        self.requests_per_minute = requests_per_minute
        self.request_times = []

    async def enqueue(self, request_fn):
        future = asyncio.Future()
        self.queue.append((request_fn, future))

        if not self.processing:
            asyncio.create_task(self.process_queue())

        return await future

    async def process_queue(self):
        self.processing = True

        while self.queue:
            # Remove old request times (older than 1 minute)
            one_minute_ago = time.time() - 60
            self.request_times = [t for t in self.request_times if t > one_minute_ago]

            # Wait if we've hit the rate limit
            if len(self.request_times) >= self.requests_per_minute:
                oldest_request = self.request_times[0]
                wait_time = 60 - (time.time() - oldest_request)
                await asyncio.sleep(wait_time)
                continue

            # Process next request
            request_fn, future = self.queue.popleft()
            self.request_times.append(time.time())

            try:
                result = request_fn()
                future.set_result(result)
            except Exception as error:
                future.set_exception(error)

        self.processing = False

# Usage
queue = RequestQueue(60)  # 60 RPM

for i in range(100):
    await queue.enqueue(
        lambda: client.chat.completions.create(
            model="grok-2",
            messages=[{"role": "user", "content": f"Message {i}"}]
        )
    )
```

</CodeGroup>

## Best Practices

### 1. Always Implement Retry Logic

```typescript
// ❌ Bad: No retry logic
const completion = await client.chat.completions.create({
  model: 'grok-2',
  messages: [{ role: 'user', content: 'Hello' }]
});

// ✅ Good: Exponential backoff retry
const completion = await makeRequestWithBackoff(() =>
  client.chat.completions.create({
    model: 'grok-2',
    messages: [{ role: 'user', content: 'Hello' }]
  })
);
```

### 2. Use Request Queues for Batch Operations

```typescript
// ❌ Bad: Fire all requests at once
for (const message of messages) {
  await client.chat.completions.create({ ... });
}

// ✅ Good: Queue requests to stay within limits
const queue = new RequestQueue(60);
for (const message of messages) {
  queue.enqueue(() => client.chat.completions.create({ ... }));
}
```

### 3. Monitor Rate Limit Headers

```typescript
// Track rate limit headers
const response = await fetch('https://api.outcryai.com/v1/chat/completions', {
  headers: { 'Authorization': `Bearer ${apiKey}` }
});

const remaining = parseInt(response.headers.get('X-RateLimit-Remaining'));
const resetTime = parseInt(response.headers.get('X-RateLimit-Reset'));

console.log(`${remaining} requests remaining until ${new Date(resetTime * 1000)}`);
```

### 4. Implement Circuit Breaker Pattern

```typescript
class CircuitBreaker {
  private failureCount = 0;
  private lastFailureTime = 0;
  private state: 'closed' | 'open' | 'half-open' = 'closed';
  private readonly threshold = 5;
  private readonly resetTimeout = 60000; // 1 minute

  async execute<T>(requestFn: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailureTime > this.resetTimeout) {
        this.state = 'half-open';
      } else {
        throw new Error('Circuit breaker is open');
      }
    }

    try {
      const result = await requestFn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess() {
    this.failureCount = 0;
    this.state = 'closed';
  }

  private onFailure() {
    this.failureCount++;
    this.lastFailureTime = Date.now();

    if (this.failureCount >= this.threshold) {
      this.state = 'open';
    }
  }
}
```

### 5. Cache Responses

```typescript
const cache = new Map();

async function cachedRequest(cacheKey: string, requestFn: () => Promise<any>) {
  if (cache.has(cacheKey)) {
    return cache.get(cacheKey);
  }

  const result = await requestFn();
  cache.set(cacheKey, result);
  return result;
}

// Usage
const completion = await cachedRequest(
  'greeting',
  () => client.chat.completions.create({
    model: 'grok-2',
    messages: [{ role: 'user', content: 'Hello' }]
  })
);
```

## Upgrading Tiers

Need higher limits? Upgrade your organization tier:

| From | To | Benefits | Cost |
|------|----|-----------| -----|
| Free → Standard | 10 → 60 RPM | 6x more requests, 100x more tokens | $100/mo |
| Standard → Premium | 60 → 300 RPM | 5x more requests, 10x more tokens | $500/mo |
| Premium → Enterprise | Custom | Unlimited, dedicated support | Custom |

**Contact sales** for Enterprise tier: sales@outcryai.com

## Rate Limit FAQs

### Do rate limits reset exactly at the minute mark?

No, rate limits use a **sliding window** algorithm. Each request tracks its own 60-second window.

### What happens if I exceed the token daily limit?

You'll receive a `429` error with `code: "daily_token_limit_exceeded"`. The limit resets at midnight UTC.

### Do failed requests count toward rate limits?

Yes, all requests count toward rate limits, including failed requests (4xx, 5xx errors).

### Can I request a rate limit increase?

Yes! Contact support@outcryai.com with your use case and we can adjust limits or recommend a tier upgrade.

### Do streaming requests count as multiple requests?

No, streaming counts as a single request regardless of duration.

## Next Steps

<CardGroup cols={2}>
  <Card title="Error Handling" icon="triangle-exclamation" href="/guides/error-handling">
    Learn comprehensive error handling
  </Card>

  <Card title="Best Practices" icon="lightbulb" href="/guides/best-practices">
    Optimize API usage
  </Card>

  <Card title="Authentication" icon="key" href="/guides/authentication">
    Manage API keys and scopes
  </Card>

  <Card title="Webhooks" icon="webhook" href="/guides/webhooks">
    Set up webhooks for async operations
  </Card>
</CardGroup>
